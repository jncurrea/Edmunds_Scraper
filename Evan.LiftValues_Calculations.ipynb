{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a36269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid line: word,count\n",
      "Top 10 brands: ['bmw', 'acura', 'infiniti', 'audi', 'lexus', 'honda', 'toyota', 'nissan', 'volkswagen', 'ford']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m2/b8l4_05d1p39hmx00f4y_czr0000gn/T/ipykernel_19970/3190228237.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top 10 brands:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_brands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# Process the input file and calculate lift values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0mposts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_input_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m \u001b[0mdf_lift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_lift_for_brands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_brands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;31m# Save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_lift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m2/b8l4_05d1p39hmx00f4y_czr0000gn/T/ipykernel_19970/3190228237.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(posts, top_brands)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mp_word1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_frequency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mp_word2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_frequency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp_word1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp_word2\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlift_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_word1_and_word2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp_word1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp_word2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mdf_lift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_lift\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'word1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lift_value'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlift_value\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_lift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# File paths\n",
    "wordcounts_file = 'word_counts.txt'\n",
    "replacement_words_file = 'replacement_words.csv'\n",
    "output_lift_values = 'Lift_Values.csv'\n",
    "output_lift_matrix = 'Lift_Matrix.csv'\n",
    "input_file = 'replacement_sample_data.csv'\n",
    "pair_keys_file = 'edmunds_pair_keys.txt'\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords from nltk if not already done\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing punctuation, converting to lowercase, and removing stopwords.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return cleaned_words  # Return the cleaned words as a list\n",
    "\n",
    "# Step 1: Load and sort word counts\n",
    "def load_word_counts(filename):\n",
    "    \"\"\"\n",
    "    Loads word counts from a text file and returns a dictionary of word:count pairs.\n",
    "    Skips any lines that don't have valid counts.\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                word, count = line.strip().split(',')\n",
    "                word_counts[word] = int(count)\n",
    "            except ValueError:\n",
    "                # Skip lines with invalid data (like headers or incorrectly formatted lines)\n",
    "                print(f\"Skipping invalid line: {line.strip()}\")\n",
    "    # Sort word counts by frequency (highest to lowest)\n",
    "    sorted_word_counts = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "    return sorted_word_counts\n",
    "\n",
    "\n",
    "# Step 2: Load replacement words (brands and models)\n",
    "def load_replacement_words(filename):\n",
    "    \"\"\"\n",
    "    Loads replacement words from a CSV file, returns a set of valid brands,\n",
    "    and excludes the words 'car' and 'seat'.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    valid_brands = set(df['Brand'].unique())  # Get unique brands\n",
<<<<<<< HEAD
    "    # Exclude non car brands from the file\n",
=======
    "    # Exclude all non brands from replacement.words.csv for validation\n",
>>>>>>> 448e59eb4f8b3fe190c1d132131926464930176a
    "    valid_brands.discard('car')\n",
    "    valid_brands.discard('seat')\n",
    "    valid_brands.discard('problem')\n",
    "    valid_brands.discard('sedan')\n",
    "    return valid_brands\n",
    "\n",
    "\n",
    "# Step 3: Get top 10 brands by word count\n",
    "def get_top_brands(word_counts, valid_brands):\n",
    "    \"\"\"\n",
    "    Matches the words in word_counts to valid brands and returns up to the top 10 brands by count.\n",
    "    Ensures that only car brands (from valid_brands) are included, ignoring non-brand words.\n",
    "    Stops either when 10 brands are found or when the end of the word_counts list is reached.\n",
    "    \"\"\"\n",
    "    top_brands = []\n",
    "    for word, count in word_counts.items():\n",
    "        # Only add the word if it's in the valid_brands set\n",
    "        if word in valid_brands:\n",
    "            top_brands.append(word)\n",
    "        # Stop if we have found 10 brands or processed all words\n",
    "        if len(top_brands) == 10:\n",
    "            break\n",
    "    \n",
    "    return top_brands\n",
    "\n",
    "# Step 4: Process the input CSV file to extract posts\n",
    "def process_input_file(input_filename):\n",
    "    \"\"\"\n",
    "    Processes the input CSV file to extract and clean posts. Each post is\n",
    "    tokenized and cleaned of punctuation and stopwords, and stored in a list.\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    with open(input_filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            post = clean_text(row[1])  # Assuming the text is in the first column\n",
    "            posts.append(post)\n",
    "    return posts\n",
    "\n",
    "# Step 5: Calculate word pair lift values\n",
    "def calculate_lift_for_brands(posts, top_brands):\n",
    "    \"\"\"\n",
    "    Calculates lift values for the top brands.\n",
    "    \"\"\"\n",
    "    word_frequency = defaultdict(int)\n",
    "    word_pair_frequency = defaultdict(lambda: defaultdict(int))\n",
    "    total_posts = len(posts)\n",
    "\n",
    "    # Count frequencies of brands and co-occurrences\n",
    "    for post in posts:\n",
    "        unique_words = set(post)\n",
    "        for word in unique_words:\n",
    "            if word in top_brands:\n",
    "                word_frequency[word] += 1\n",
    "        for word1 in unique_words:\n",
    "            for word2 in unique_words:\n",
    "                if word1 in top_brands and word2 in top_brands and word1 != word2:\n",
    "                    word_pair_frequency[word1][word2] += 1\n",
    "\n",
    "    # Calculate lift values and store them in a DataFrame\n",
    "    df_lift = pd.DataFrame(columns=['word1', 'word2', 'lift_value'])\n",
    "    for word1 in top_brands:\n",
    "        for word2 in top_brands:\n",
    "            if word1 != word2:\n",
    "                p_word1_and_word2 = word_pair_frequency[word1][word2] / total_posts\n",
    "                p_word1 = word_frequency[word1] / total_posts\n",
    "                p_word2 = word_frequency[word2] / total_posts\n",
    "                if p_word1 > 0 and p_word2 > 0:\n",
    "                    lift_value = p_word1_and_word2 / (p_word1 * p_word2)\n",
    "                    df_lift = df_lift.append({'word1': word1, 'word2': word2, 'lift_value': lift_value}, ignore_index=True)\n",
    "    \n",
    "    return df_lift\n",
    "\n",
    "# Step 6: Save results\n",
    "def save_results(df_lift):\n",
    "    \"\"\"\n",
    "    Saves the lift values and matrix to CSV files.\n",
    "    \"\"\"\n",
    "    df_lift.to_csv(output_lift_values, index=False)\n",
    "    lift_matrix = df_lift.pivot(index='word1', columns='word2', values='lift_value')\n",
    "    lift_matrix.to_csv(output_lift_matrix)\n",
    "\n",
    "# Main script\n",
    "\n",
    "# Load word counts and replacement words\n",
    "word_counts = load_word_counts(wordcounts_file)\n",
    "valid_brands = load_replacement_words(replacement_words_file)\n",
    "\n",
    "# Get the top 10 brands by word count\n",
    "top_brands = get_top_brands(word_counts, valid_brands)\n",
    "print(\"Top 10 brands:\", top_brands)\n",
    "\n",
    "# Process the input file and calculate lift values\n",
    "posts = process_input_file(input_file)\n",
    "df_lift = calculate_lift_for_brands(posts, top_brands)\n",
    "\n",
    "# Save results\n",
    "save_results(df_lift)\n",
    "\n",
    "print('output_lift_matrix.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
